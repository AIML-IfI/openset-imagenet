
---
# General Parameters
exp_name: experiment
checkpoint:         # Relative path from the original working directory
log_name: training.log  # name of the file with training info
train_mode: train  # either: {train, finetune}

data:
  imagenet_path: /local/scratch/datasets/ImageNet/ILSVRC2012/
  data_dir: ${hydra:runtime.cwd}/data
  train_file: p1_train.csv        # relative to data directory
  val_file:   p1_val.csv          # relative to data directory
  test_file:  p1_test.csv         # relative to data directory

loss:
  type: entropic  # either {objectosphere, entropic, softmax}
  w: 1.0          # Weight of unknown samples in entropic
  xi: 0.0         # Applies for objectosphere
  alpha: 0.0      # Applies for objectosphere

opt:
  type: adam  # either {adam, sgd}
  lr: 1e-3
  decay: 0    # number of epochs to wait before decreasing the learning rate, if 0, no decay
  gamma: 1    # Factor to decay the learning rate

# Common parameters
seed: 42    # Sets torch and numpy seeds
batch_size: 64  # if distributed the batch size is multiplied for the number of dist.gpus
epochs: 100
workers: 4
threshold: 0.5
patience: 0     # Number of epochs to wait. It must be > 0

adv:
  who: no_adv   # [no_adv, gaussian, random, fgsm]
  epsilon: 0.0  # For [fgsm,random]
  std: 0.00     # For gaussian
  mode: filter  # [filter, full]
  mu: 1.0       # Factor to decay epsilon every eps_n epochs
  decay: 0      # Epochs to wait to decrease epsilon by mu
  wait: 0       # Epochs to train with only knowns, before adding adversarials

# Parameters for distributed training
dist:
  distributed: False
  gpus: 1
  port: 8889

# Hydra Parameters
hydra:
  run:
    dir: ./outputs/${exp_name}
defaults:
  - override hydra/job_logging: none
