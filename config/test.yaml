# General Parameters
name: experiment
checkpoint:             # Relative path to checkpoint
log_name: testing.log  # name of the file with training info
train_mode: train       # either: {train, finetune}
gpu:                    # index of a GPU, if available
parallel: off           # run in parallel

# Data Parameters
data:
  imagenet_path: /local/scratch/datasets/ImageNet/ILSVRC2012/ # ILSVRC2012 path
  #train_file: protocols/p{}_train.csv        # relative to data directory
  val_file:   protocols/p{}_val.csv          # relative to data directory
  test_file:  protocols/p{}_test.csv         # relative to data directory

# Common parameters
seed: 42        # Common seed across all source of randomness
batch_size: 64  # If distributed training the batch size is multiplied by the number of gpus
epochs: 50
workers: 4      # Dataloader number of workers
patience: 0     # Number of epochs to wait before stopping the training. 0 means no early stopping

loss:
  type:  # either {entropic, softmax, garbage} #comes from the command line args. Leaving empty
  # Entropic Parameters
  w: 1.

# Optimizer Parameters
opt:
  type: adam  # Two options: {adam, sgd}
  lr: 1.e-3   # Initial learning rate
  decay: 0    # Number of epochs to wait for each learning rate reduction. 0 means no decay
  gamma: 1    # Factor to reduce the learning rate

# Parameters for Distributed Data Parallel training
dist:
  distributed: True  # False to use only one GPU # TODO: delete this option
  gpus: 2            # Number of GPUS to use
  port: "8889"       # Default port to communicate.


alg_dict:
  openmax: OpenMax
  evm: EVM
  proser: Proser
  threshold: threshold

#dnn_features: '/home/user/halil/master-suter/code/eosa/eosa/experiments/results/models/p1_traincls(kk)_baseline_resnet50_feature_df512_e200_optAdam(0.001+0.7+0.999).pth'

#DNN model to use
#dnn_features: "experiments/Protocol_1/softmax_best.pth"

openmax_model_to_test: experiments/Protocol_{}/{}_openmax_TS_750.0_DM_1.50_cosine.pkl #TS_10.0_DM_2.30
evm_model_to_test: experiments/Protocol_{}/{}_evm_TS_100_DM_0.90_CT_1.00_cosine.pkl  #softmax_evm_TS_10_DM_1.50_CT_1.00_cosine.pkl
#model_to_test: experiments/Protocol_{}/softmax_openmax_TS_1000.0_DM_2.00_cosine.pkl

openmax_model_to_validate: experiments/Protocol_{}/{}_openmax_TS_{}_DM_{}_cosine.pkl
evm_model_to_validate: experiments/Protocol_{}/{}_evm_TS_{}_DM_{}_CT_1.00_cosine.pkl  

train_classes: [kk]

known_unknown_target: -1
unknown_unknown_target: -2

suffix: _curr  #or _best 

#below may not be needed in testing

algorithm:
  type:  #openmax evm threshold proser, it gets from the command line. No need to fill in test yaml
  base_model: experiments/Protocol_{}/{}_curr.pth #for those which use pretrained models, not needed for test

  # Proser Parameters
  lambda0: 0.1
  lambda1: 1.
  lambda2: 1.
  alpha: 1.
  dummy_count: 1

  #openmax and evm parameters. alpha is changed to alpha_om not to mix with proser's alpha. 
  tailsize: [25, 50, 75, 100, 150, 200, 300, 500, 1000] #   [10, 100, 250, 500, 750, 1000] #     
  distance_multiplier: [0.20, 0.30, 0.40, 0.50, 0.70, 0.90, 1.00] # [1.50, 1.7, 2.0, 2.3] #     
  translateAmount: [1]
  distance_metric: cosine
  alpha_om: [2, 3, 5, 10]
  cover_threshold: [1.00] # specific to evm
  chunk_size: 100 #specific to evm

validate_on: False
eval_on: True #validation with a specific model
test_on: True
fpr_thresholds: [0.001, 0.01, 0.1, 0.15, 0.2]
